# Dataset Generation Pipeline 🛠️⚙️

This package provides a code generation pipeline for gathering the relevant datasets for the experiments in the paper. The pipeline supports models hosted on Hugging Face, OpenAI, and AWS Bedrock, and processes datasets from different sources to generate, sanitize, and store results.

### API Keys 🔑
This pipeline requires access to OpenAI and AWS Bedrock. Ensure you have valid API credentials set up:

1. **OpenAI API**: Store your API key in a `keys.py` file:
   ```python
   OPENAI_API_KEY = "your-openai-api-key"
   ```

2. **AWS Bedrock API**: Store your AWS credentials in `keys.py`:
   ```python
   AWS_ACCESS_KEY_ID = "your-access-key"
   AWS_SECRET_ACCESS_KEY = "your-secret-key"
   ```

## Usage 🚀
Here is an example on how to run the pipeline:

```bash
python main.py --datasets google-research-datasets/mbpp --models codellama/CodeLlama-7b-Instruct-hf gpt-4o-mini
```

#### Arguments 📝
- `--datasets`: List of datasets to use. Options:
  - `codeparrot/apps`
  - `google-research-datasets/mbpp`
  - `deepmind/code_contests_java`
  - `deepmind/code_contests_cpp`
- `--models`: List of models to use. Options:
  - `HuggingFaceH4/starchat-alpha`
  - `gpt-3.5-turbo`
  - `gpt-4o-mini`
  - `codellama/CodeLlama-13b-Instruct-hf`
  - `codellama/CodeLlama-7b-Instruct-hf`
  - `google/codegemma-7b-it`
  - `claude3-haiku`
- `--temperature`: Temperature setting for generation (default: 0.7).


## Output Structure 📂
- **Sanitized Dataset**: `results/{dataset}/{dataset}_sanitized.json`
- **Generated Responses**: `results/{dataset}/{dataset}_{model}_responses.jsonl`

### Example Output 📄
Generated model response example (saved in `results/{dataset}/{dataset}_{model}_responses.jsonl`):
For the MBPP dataset, this is an example entry in the tasks `.jsonl` file:
```json
{
   "task_id":19,
   "text":"Write a function to find whether a given array of integers contains any duplicate element.",
   "code":"def test_duplicate(arraynums):\r\n    nums_set = set(arraynums)    \r\n    return len(arraynums) != len(nums_set)     ",
   "test_list":[
      "assert test_duplicate(([1,2,3,4,5]))==False",
      "assert test_duplicate(([1,2,3,4, 4]))==True",
      "assert test_duplicate([1,1,2,2,3,3,4,4,5])==True"
   ],
   "test_setup_code":"",
   "challenge_test_list":[
      
   ],
   "raw_model_response":"  ```\ndef contains_duplicate(nums):\n    return len(nums)!= len(set(nums))\n```\nExplanation:\n\nThe `set()` function creates a set from an iterable, which in this case is the `nums` array. The `len()` function returns the length of the set, which is the number of unique elements in the array.\n\nIf the array contains any duplicate elements, the length of the set will be less than the length of the array. In this case, the `len(nums)!= len(set(nums))` condition will be true, indicating that the array contains a duplicate element.\n\nThe function returns `True` if the condition is met, and `False` otherwise.\n\nFor example, if we call the function with the argument `nums = [1, 2, 3, 4, 5]`, it will return `False`, because the array does not contain any duplicate elements.\n\nIf we call the function with the argument `nums = [1, 2, 3, 4, 5, 5]`, it will return `True`, because the array contains a duplicate element (the number 5).",
   "model_response":"def contains_duplicate(nums):\n    return len(nums)!= len(set(nums))"
}
```

## Generated Datasets 📂
We strongly believe in releasing the generated datasets we used in the experiments.
However, we need to review the terms of agreements and usage requirements of the different generator-LLMs we used, to ensure it is permissible to publish code generated by these models.  

Note that even if we set a seed, proprietary models such as GPT and Claude include some level of randomness due to their API-based nature.

Once we make sure there are no legal issues with releasing the generated datasets, we will upload them to a publicly-available repository.
